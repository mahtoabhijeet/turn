{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1757812380010,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "ggs8ZwbGx3_9",
    "outputId": "b7ade112-35d4-4cc8-d5fc-11a64a9932e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings shape: torch.Size([1, 3, 128])\n",
      "‚úÖ Generated from only 4 integers per token!\n",
      "Turn values for 'cat': [ 1. -3.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Turn Embeddings: From 4 Numbers to 768-Dim Vectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class TurnEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, n_turns=4, output_dim=128, poly_degree=3):\n",
    "        super().__init__()\n",
    "        # Each word = 4 integers (your \"hydrogen atoms\" of meaning)\n",
    "        self.turns = nn.Parameter(torch.randint(-5, 6, (vocab_size, n_turns)).float())\n",
    "        # Polynomial coefficients: the \"creative wormholes\"\n",
    "        self.poly_coeffs = nn.Parameter(torch.randn(n_turns, poly_degree + 1, output_dim) * 0.1)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        base_turns = self.turns[token_ids]  # [B, S, 4]\n",
    "        embeddings = torch.zeros(*base_turns.shape[:2], self.poly_coeffs.size(-1))\n",
    "\n",
    "        for i in range(self.poly_coeffs.size(0)):  # for each turn space\n",
    "            x = base_turns[..., i].unsqueeze(-1)  # [B, S, 1]\n",
    "            # Generate polynomial: 1, x, x¬≤, x¬≥\n",
    "            powers = torch.cat([x**d for d in range(self.poly_coeffs.size(1))], dim=-1)  # [B, S, deg+1]\n",
    "            # Apply coefficients ‚Üí [B, S, output_dim]\n",
    "            embeddings += torch.einsum('bsd,do->bso', powers, self.poly_coeffs[i])\n",
    "        return embeddings\n",
    "\n",
    "# --- Test it ---\n",
    "vocab_size = 100\n",
    "turn_emb = TurnEmbedding(vocab_size, n_turns=4, output_dim=128)\n",
    "\n",
    "# Create a tiny vocab for testing\n",
    "vocab = {word: i for i, word in enumerate([\"cat\", \"dog\", \"king\", \"queen\", \"man\", \"woman\", \"red\", \"blue\"])}\n",
    "tokens = torch.tensor([[vocab[\"cat\"], vocab[\"king\"], vocab[\"red\"]]])  # batch=1, seq=3\n",
    "embeddings = turn_emb(tokens)\n",
    "\n",
    "print(\"‚úÖ Embeddings shape:\", embeddings.shape)  # Should be [1, 3, 128]\n",
    "print(\"‚úÖ Generated from only 4 integers per token!\")\n",
    "print(\"Turn values for 'cat':\", turn_emb.turns[vocab[\"cat\"]].detach().numpy().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6319,
     "status": "ok",
     "timestamp": 1757812609104,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "1xrQOJLfycTj",
    "outputId": "f76a86ad-10b4-418c-d72e-c811c8fa140e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 156.3811\n",
      "Turns for 'cat': [ 0.99 -2.99 -0.01  0.01]\n",
      "Epoch 20, Loss: 7.9085\n",
      "Turns for 'cat': [ 1.15 -2.85 -0.16  0.2 ]\n",
      "Epoch 40, Loss: 2.0660\n",
      "Turns for 'cat': [ 1.13 -2.8  -0.14  0.38]\n",
      "Epoch 60, Loss: 1.1053\n",
      "Turns for 'cat': [ 1.15 -2.79 -0.15  0.57]\n",
      "Epoch 80, Loss: 0.9700\n",
      "Turns for 'cat': [ 1.14 -2.79 -0.17  0.7 ]\n"
     ]
    }
   ],
   "source": [
    "# @title Phase 2: Train Turns to Reconstruct Semantic Space\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Simulate \"ground truth\" embeddings (e.g., from Word2Vec)\n",
    "np.random.seed(42)\n",
    "simulated_embeddings = np.random.randn(100, 128)  # 100 words, 128-dim\n",
    "\n",
    "# Add 'car' to the vocabulary\n",
    "vocab[\"car\"] = len(vocab)\n",
    "\n",
    "# Make \"cat\" and \"dog\" similar, \"cat\" and \"car\" dissimilar\n",
    "# Replace [...] with actual 128-dimensional vectors\n",
    "\n",
    "# Option 1: Use random but correlated vectors\n",
    "base_cat = np.random.randn(128)\n",
    "simulated_embeddings[vocab[\"cat\"]] = base_cat\n",
    "simulated_embeddings[vocab[\"dog\"]] = base_cat + np.random.randn(128) * 0.1  # Slightly perturbed\n",
    "simulated_embeddings[vocab[\"car\"]] = -base_cat + np.random.randn(128) * 0.5  # Roughly opposite\n",
    "\n",
    "# Option 2: Use structured patterns (for debugging)\n",
    "# simulated_embeddings[vocab[\"cat\"]] = np.array([0.9] * 64 + [0.1] * 64)  # First half high, second half low\n",
    "# simulated_embeddings[vocab[\"dog\"]] = np.array([0.85] * 64 + [0.15] * 64)\n",
    "# simulated_embeddings[vocab[\"car\"]] = np.array([-0.8] * 64 + [0.9] * 64)\n",
    "\n",
    "simulated_embeddings = torch.tensor(simulated_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(turn_emb.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get current embeddings for all words\n",
    "    all_tokens = torch.arange(100).unsqueeze(0)  # [1, 100]\n",
    "    current_embeddings = turn_emb(all_tokens)  # [1, 100, 128]\n",
    "\n",
    "    # Compute loss: match simulated embeddings\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        current_embeddings.squeeze(0),\n",
    "        simulated_embeddings\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "        print(f\"Turns for 'cat': {turn_emb.turns[vocab['cat']].detach().numpy().round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1757812723219,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "l8mff95XzXZw",
    "outputId": "6540938d-59f8-4759-f89e-c88768188589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê± Computed 'kitten' (cat + small): [ 1.14 -2.79 -1.7   0.79]\n",
      "üß∏ Actual 'kitten' turns:           [ 1.1 -2.8 -1.8  0.6]\n",
      "üìè Distance: 0.213380366563797\n"
     ]
    }
   ],
   "source": [
    "# @title Test Turn Arithmetic: Does \"cat\" + \"small\" = \"kitten\"?\n",
    "\n",
    "# Simulate \"small\" ‚Äî you can initialize it or let it train\n",
    "vocab[\"small\"] = 1  # assuming index 1\n",
    "turn_emb.turns.data[vocab[\"small\"]] = torch.tensor([0.0, 0.0, -1.5, 0.0])  # manually set\n",
    "\n",
    "# Simulate \"kitten\"\n",
    "vocab[\"kitten\"] = 2\n",
    "turn_emb.turns.data[vocab[\"kitten\"]] = torch.tensor([1.1, -2.8, -1.8, 0.6])  # ground truth\n",
    "\n",
    "# Compute: cat + small\n",
    "cat_turns = turn_emb.turns[vocab[\"cat\"]].detach()\n",
    "small_turns = turn_emb.turns[vocab[\"small\"]].detach()\n",
    "computed_kitten_turns = cat_turns + small_turns\n",
    "\n",
    "actual_kitten_turns = turn_emb.turns[vocab[\"kitten\"]].detach()\n",
    "\n",
    "print(\"üê± Computed 'kitten' (cat + small):\", computed_kitten_turns.numpy().round(2))\n",
    "print(\"üß∏ Actual 'kitten' turns:          \", actual_kitten_turns.numpy().round(2))\n",
    "print(\"üìè Distance:\", torch.norm(computed_kitten_turns - actual_kitten_turns).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1757812842425,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "olBniPlsz0r_",
    "outputId": "8c658114-fedc-4f11-86ac-df4726368020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ king - man + woman = queen | Distance: 10.555\n",
      "‚ö†Ô∏è  Missing word in vocab: cat, small, big, or lion\n",
      "‚ö†Ô∏è  Missing word in vocab: hot, temperature, cold, or ice\n",
      "‚ö†Ô∏è  Missing word in vocab: run, present, past, or ran\n"
     ]
    }
   ],
   "source": [
    "# @title Automated Turn Arithmetic Test Suite\n",
    "test_cases = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"cat\", \"small\", \"big\", \"lion\"),  # cat - small + big = lion?\n",
    "    (\"hot\", \"temperature\", \"cold\", \"ice\"),  # Semantic field transfer\n",
    "    (\"run\", \"present\", \"past\", \"ran\"),  # Temporal modulation\n",
    "]\n",
    "\n",
    "def run_turn_arithmetic_tests(model, vocab, test_cases):\n",
    "    word_turns = model.turns.data\n",
    "    results = []\n",
    "\n",
    "    for a, b, c, expected_d in test_cases:\n",
    "        try:\n",
    "            turn_a = word_turns[vocab[a]]\n",
    "            turn_b = word_turns[vocab[b]]\n",
    "            turn_c = word_turns[vocab[c]]\n",
    "            expected_turn_d = word_turns[vocab[expected_d]]\n",
    "\n",
    "            computed_d = turn_a - turn_b + turn_c\n",
    "            distance = torch.norm(computed_d - expected_turn_d).item()\n",
    "\n",
    "            results.append({\n",
    "                'test': f\"{a} - {b} + {c} = {expected_d}\",\n",
    "                'computed': computed_d.numpy().round(2).tolist(),\n",
    "                'actual': expected_turn_d.numpy().round(2).tolist(),\n",
    "                'distance': round(distance, 3)\n",
    "            })\n",
    "\n",
    "            print(f\"‚úÖ {a} - {b} + {c} = {expected_d} | Distance: {distance:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"‚ö†Ô∏è  Missing word in vocab: {a}, {b}, {c}, or {expected_d}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run it\n",
    "results = run_turn_arithmetic_tests(turn_emb, vocab, test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757812945219,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "VDj3AkFa0N0C"
   },
   "outputs": [],
   "source": [
    "# @title Fix: Build a Semantic Vocabulary with Turn Initialization\n",
    "\n",
    "# Define your expanded vocabulary\n",
    "expanded_vocab = {\n",
    "    \"king\": 0, \"queen\": 1, \"man\": 2, \"woman\": 3,\n",
    "    \"cat\": 4, \"dog\": 5, \"kitten\": 6, \"lion\": 7, \"small\": 8, \"big\": 9,\n",
    "    \"hot\": 10, \"cold\": 11, \"ice\": 12, \"temperature\": 13,\n",
    "    \"run\": 14, \"ran\": 15, \"present\": 16, \"past\": 17,\n",
    "    # Add more as needed\n",
    "}\n",
    "\n",
    "# Initialize turns SEMANTICALLY (not randomly!)\n",
    "# Format: [Conceptual Essence, Behavioral Axis, Size/Scale, Context/Temporal]\n",
    "semantic_turns_init = torch.tensor([\n",
    "    # Royalty & Gender\n",
    "    [5.0, 0.0, 0.0, 0.0],  # king\n",
    "    [5.0, 0.0, 0.0, 0.0],  # queen (same essence, gender handled by context)\n",
    "    [2.0, 0.0, 0.0, 0.0],  # man\n",
    "    [2.0, 0.0, 0.0, 0.0],  # woman\n",
    "\n",
    "    # Animals & Size\n",
    "    [3.0, -2.0, 0.0, 0.0],  # cat (independent)\n",
    "    [3.0, 2.0, 0.0, 0.0],   # dog (social)\n",
    "    [3.0, -2.0, -2.0, 0.0], # kitten (small cat)\n",
    "    [3.0, -2.0, 3.0, 0.0],  # lion (large cat)\n",
    "    [0.0, 0.0, -3.0, 0.0],  # small\n",
    "    [0.0, 0.0, 3.0, 0.0],   # big\n",
    "\n",
    "    # Temperature\n",
    "    [0.0, 0.0, 0.0, 4.0],   # hot\n",
    "    [0.0, 0.0, 0.0, -4.0],  # cold\n",
    "    [0.0, 0.0, 0.0, -5.0],  # ice (very cold)\n",
    "    [0.0, 0.0, 0.0, 0.0],   # temperature (neutral base)\n",
    "\n",
    "    # Verbs & Tense\n",
    "    [1.0, 0.0, 0.0, 1.0],   # run (present)\n",
    "    [1.0, 0.0, 0.0, -1.0],  # ran (past)\n",
    "    [0.0, 0.0, 0.0, 1.0],   # present\n",
    "    [0.0, 0.0, 0.0, -1.0],  # past\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Update your model's turn parameters\n",
    "turn_emb.turns.data[:len(expanded_vocab)] = semantic_turns_init\n",
    "\n",
    "# Update your vocab dictionary to match\n",
    "vocab = expanded_vocab  # Overwrite your old vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1757813002640,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "crprITuB0bgF",
    "outputId": "30c285d3-2b44-44c8-e6af-1abadfed480d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.9543\n",
      "Epoch 50, Loss: 0.0411\n",
      "Epoch 100, Loss: 0.0029\n",
      "Epoch 150, Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# @title Retrain: Teach the Polynomial to Respect Your Semantic Turns\n",
    "\n",
    "# Simulate \"ground truth\" embeddings based on your semantic turns\n",
    "# We'll just use the turns themselves as a proxy for now\n",
    "simulated_embeddings = semantic_turns_init.repeat(1, 32)[:, :128]  # Expand to 128-dim\n",
    "\n",
    "# Make sure it's a tensor\n",
    "simulated_embeddings = simulated_embeddings.clone().detach().requires_grad_(False)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(turn_emb.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):  # Train longer this time\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Get current embeddings for all words in your vocab\n",
    "    all_tokens = torch.arange(len(expanded_vocab)).unsqueeze(0)  # [1, vocab_size]\n",
    "    current_embeddings = turn_emb(all_tokens)  # [1, vocab_size, 128]\n",
    "\n",
    "    # Compute loss: match your semantic structure\n",
    "    loss = torch.nn.functional.mse_loss(\n",
    "        current_embeddings.squeeze(0),\n",
    "        simulated_embeddings\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1757813028957,
     "user": {
      "displayName": "abhijeet mahto",
      "userId": "14190744274354055211"
     },
     "user_tz": -330
    },
    "id": "UTAnpkf10iRv",
    "outputId": "41034f10-3045-42b3-a06a-7b74c0c162f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ king - man + woman = queen | Distance: 0.000\n",
      "   Computed: [ 4.8   0.07  0.01 -0.04]\n",
      "   Expected: [ 4.8   0.07  0.01 -0.04]\n",
      "\n",
      "‚úÖ cat - small + big = lion | Distance: 2.463\n",
      "   Computed: [ 2.85 -1.95  5.4   0.09]\n",
      "   Expected: [ 3.1  -2.08  2.96  0.26]\n",
      "\n",
      "‚úÖ hot - temperature + cold = ice | Distance: 5.184\n",
      "   Computed: [ 1.48 -0.77  0.91  0.06]\n",
      "   Expected: [ 0.1  -0.07 -0.2  -4.76]\n",
      "\n",
      "‚úÖ run - present + past = ran | Distance: 0.180\n",
      "   Computed: [ 1.67 -0.49  0.89 -1.91]\n",
      "   Expected: [ 1.77 -0.58  1.   -1.89]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Validate: Test All Your Semantic Arithmetic\n",
    "\n",
    "test_cases = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"cat\", \"small\", \"big\", \"lion\"),    # Now in vocab!\n",
    "    (\"hot\", \"temperature\", \"cold\", \"ice\"),  # Now in vocab!\n",
    "    (\"run\", \"present\", \"past\", \"ran\"),      # Now in vocab!\n",
    "]\n",
    "\n",
    "def run_turn_arithmetic_tests(model, vocab, test_cases):\n",
    "    word_turns = model.turns.data\n",
    "    results = []\n",
    "\n",
    "    for a, b, c, expected_d in test_cases:\n",
    "        try:\n",
    "            turn_a = word_turns[vocab[a]]\n",
    "            turn_b = word_turns[vocab[b]]\n",
    "            turn_c = word_turns[vocab[c]]\n",
    "            expected_turn_d = word_turns[vocab[expected_d]]\n",
    "\n",
    "            computed_d = turn_a - turn_b + turn_c\n",
    "            distance = torch.norm(computed_d - expected_turn_d).item()\n",
    "\n",
    "            results.append({\n",
    "                'test': f\"{a} - {b} + {c} = {expected_d}\",\n",
    "                'computed': computed_d.numpy().round(2).tolist(),\n",
    "                'actual': expected_turn_d.numpy().round(2).tolist(),\n",
    "                'distance': round(distance, 3)\n",
    "            })\n",
    "\n",
    "            print(f\"‚úÖ {a} - {b} + {c} = {expected_d} | Distance: {distance:.3f}\")\n",
    "            print(f\"   Computed: {computed_d.numpy().round(2)}\")\n",
    "            print(f\"   Expected: {expected_turn_d.numpy().round(2)}\\n\")\n",
    "        except KeyError as e:\n",
    "            print(f\"‚ö†Ô∏è  STILL Missing: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run it!\n",
    "results = run_turn_arithmetic_tests(turn_emb, vocab, test_cases)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNoHKcHmFKRTKKAT4TM/2m+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
